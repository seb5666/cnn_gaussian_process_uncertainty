\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Gal2016Uncertainty}
\citation{Goodfellow-et-al-2016}
\citation{gal2016dropout}
\citation{gal2016dropout}
\citation{lecun1998gradient}
\citation{Goodfellow-et-al-2016}
\citation{DBLP:journals/corr/GuWKMSSLWW15}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background}{1}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Convolutional neural networks}{1}{subsection.2.1}}
\citation{adam}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Gaussian processes}{2}{subsection.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Regression}{2}{subsubsection.2.2.1}}
\citation{villacampa2017scalable}
\citation{Goodfellow-et-al-2016}
\citation{gal2016dropout}
\citation{Gal2016Uncertainty}
\citation{Gal2016Uncertainty}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Classification}{3}{subsubsection.2.2.2}}
\@writefile{toc}{\contentsline {paragraph}{Binary classification}{3}{section*.1}}
\@writefile{toc}{\contentsline {paragraph}{Multi-class classification}{3}{section*.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Uncertainty in Deep Learning}{3}{subsection.2.3}}
\citation{chollet2015keras}
\citation{srivastava2014dropout}
\citation{hensman2015scalable}
\citation{GPflow2017}
\citation{hensman2015scalable}
\citation{rasmussen2006gaussian}
\citation{chow1970optimum}
\@writefile{toc}{\contentsline {section}{\numberline {3}Method}{4}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Convolutional Neural Network architecture}{4}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Gaussian process}{4}{subsection.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Classifying with a reject option}{4}{subsection.3.3}}
\newlabel{section/reject_option}{{3.3}{4}{Classifying with a reject option}{subsection.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Evaluation}{5}{subsection.3.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1}Accuracy}{5}{subsubsection.3.4.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2}R-accuracy}{5}{subsubsection.3.4.2}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{5}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}MNIST dataset}{5}{subsection.4.1}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces MNIST test images accuracy for the CNN and the GP.}}{5}{table.1}}
\newlabel{table/mnist_acc}{{1}{5}{MNIST test images accuracy for the CNN and the GP}{table.1}{}}
\citation{DBLP:journals/corr/BasuKGDMN15}
\citation{Gal2016Uncertainty}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Cumulative distribution of the standard deviations for the most likely class for both correctly and incorrectly classified test images.}}{6}{figure.1}}
\newlabel{fig/mnist_cum_stds}{{1}{6}{Cumulative distribution of the standard deviations for the most likely class for both correctly and incorrectly classified test images}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Noisy MNIST (n-MNIST)}{6}{subsection.4.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Example digits from the noisy-MNIST dataset. From left to right: Additive white Gaussian noise, Motion blur, and Reduced contrast with AWGN}}{6}{figure.2}}
\newlabel{fig/noisy_mnist_example}{{2}{6}{Example digits from the noisy-MNIST dataset. From left to right: Additive white Gaussian noise, Motion blur, and Reduced contrast with AWGN}{figure.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces MNIST test images accuracy for the CNN and the GP.}}{7}{table.2}}
\newlabel{table/noisy_mnist_acc}{{2}{7}{MNIST test images accuracy for the CNN and the GP}{table.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Cumulative distribution of standard deviations for correctly and incorrectly classified test images on the 3 n-MNIST datasets.}}{7}{figure.3}}
\newlabel{fig/noisy_mnist_cum_stds}{{3}{7}{Cumulative distribution of standard deviations for correctly and incorrectly classified test images on the 3 n-MNIST datasets}{figure.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Classifying with a reject option}{7}{subsection.4.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Left: R-accuracy obtained using varying $\epsilon $ and $\alpha $ but setting $\beta =0$. For $\alpha >0$ there seems to be an optimal value of $\epsilon $ around 30. Right: Number of examples misclassified or rejected. This illustrates the trade-off between the number of misclassified but accepted and correctly classified but rejected examples.}}{7}{figure.4}}
\newlabel{fig/r_accs_alpha_mnist}{{4}{7}{Left: R-accuracy obtained using varying $\epsilon $ and $\alpha $ but setting $\beta =0$. For $\alpha >0$ there seems to be an optimal value of $\epsilon $ around 30. Right: Number of examples misclassified or rejected. This illustrates the trade-off between the number of misclassified but accepted and correctly classified but rejected examples}{figure.4}{}}
\citation{maaten2008visualizing}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{8}{section.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Uncertainty with additive white noise}{8}{subsection.5.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Top: Test images with increasingly more additive white Gaussian noise. Bottom: Gausssian process predictions for the test images. The error bars show the standard error for each class.}}{8}{figure.5}}
\newlabel{fig/awgn_predictions}{{5}{8}{Top: Test images with increasingly more additive white Gaussian noise. Bottom: Gausssian process predictions for the test images. The error bars show the standard error for each class}{figure.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Probability and standard deviation assigned to class `6' for the 10 images shown in figure \ref  {fig/awgn_predictions}. Also plotted is the standard deviation relative to the class proability.}}{8}{figure.6}}
\newlabel{fig/awgn_predictions_stds}{{6}{8}{Probability and standard deviation assigned to class `6' for the 10 images shown in figure \ref {fig/awgn_predictions}. Also plotted is the standard deviation relative to the class proability}{figure.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}CNN features visualisation}{8}{subsection.5.2}}
\bibdata{bib}
\bibcite{Gal2016Uncertainty}{{1}{}{{}}{{}}}
\bibcite{Goodfellow-et-al-2016}{{2}{}{{}}{{}}}
\bibcite{gal2016dropout}{{3}{}{{}}{{}}}
\bibcite{lecun1998gradient}{{4}{}{{}}{{}}}
\bibcite{DBLP:journals/corr/GuWKMSSLWW15}{{5}{}{{}}{{}}}
\bibcite{adam}{{6}{}{{}}{{}}}
\bibcite{villacampa2017scalable}{{7}{}{{}}{{}}}
\bibcite{chollet2015keras}{{8}{}{{}}{{}}}
\bibcite{srivastava2014dropout}{{9}{}{{}}{{}}}
\bibcite{hensman2015scalable}{{10}{}{{}}{{}}}
\bibcite{GPflow2017}{{11}{}{{}}{{}}}
\bibcite{rasmussen2006gaussian}{{12}{}{{}}{{}}}
\bibcite{chow1970optimum}{{13}{}{{}}{{}}}
\bibcite{DBLP:journals/corr/BasuKGDMN15}{{14}{}{{}}{{}}}
\bibcite{maaten2008visualizing}{{15}{}{{}}{{}}}
\bibstyle{unsrt}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Learned features for the MNIST test dataset and the AWGN n-MNIST test dataset. The dimensionality of the features were reduced using t-SNE.}}{9}{figure.7}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\@writefile{toc}{\contentsline {section}{\numberline {A}Misclassified MNIST images by the Gaussian process model}{10}{appendix.A}}
\newlabel{appendix/mnist_error}{{A}{10}{Misclassified MNIST images by the Gaussian process model}{appendix.A}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Test images misclassified images by the GP model.}}{10}{figure.8}}
\newlabel{fig/incorrect_mnist}{{8}{10}{Test images misclassified images by the GP model}{figure.8}{}}
